# Distributed Consensus Protocol for Web4 Societies

**Date**: 2025-11-30
**Author**: Legion Autonomous Session #43
**Status**: Research prototype design - not yet implemented
**Context**: Building on Session #42's cross-platform block verification

---

## Executive Summary

This document specifies a **federation-based consensus protocol** for distributed Web4 societies, enabling multiple platforms (Thor, Sprout, Legion, etc.) to maintain a consistent blockchain while tolerating Byzantine faults.

**Key Design Principles**:
- **Build on Session #42**: Leverages Ed25519 cross-platform verification
- **Federation-based**: Platforms explicitly join consensus groups (not pure P2P)
- **Byzantine fault tolerant**: Can handle up to f malicious platforms in 3f+1 configuration
- **Incremental deployment**: Can start with 2 platforms, scale to N
- **Research prototype**: Tested at research scale, not production-deployed

---

## Background: What We Have (Session #42)

**Existing Infrastructure**:
1. âœ… Ed25519 block signing (SAGE integration)
2. âœ… Cross-platform signature verification (SageBlockVerifier + SignatureRegistry)
3. âœ… HTTP/REST federation network (Phase 3)
4. âœ… Hardware-bound platform identities

**What's Missing**: Consensus protocol for multiple platforms to agree on block order

**The Problem**:
- Thor creates block A at timestamp T
- Sprout creates block B at timestamp T
- Both blocks are valid, but they conflict
- How do we decide which to accept?

---

## Consensus Protocol: Federation-Based PBFT-Lite

### Overview

**Name**: Federation-Based Practical Byzantine Fault Tolerance (FB-PBFT)

**Inspiration**: PBFT (Practical Byzantine Fault Tolerance) simplified for federation context

**Key Difference from PBFT**:
- PBFT: Pure distributed consensus (any node can be primary)
- FB-PBFT: Federation-based (platforms explicitly join consensus groups)

**Fault Tolerance**: Tolerates up to f Byzantine faults in 3f+1 configuration
- 4 platforms: tolerates 1 fault
- 7 platforms: tolerates 2 faults
- 10 platforms: tolerates 3 faults

---

## Core Protocol: Three-Phase Commit

### Phase 1: Pre-Prepare (Block Proposal)

**Who**: Block proposer (rotates among platforms)

**What**:
1. Proposer creates block with pending transactions
2. Signs block with Ed25519 key
3. Broadcasts `PRE-PREPARE` message to all platforms

**Message Format**:
```json
{
  "type": "PRE-PREPARE",
  "view": 0,
  "sequence": 42,
  "block": {
    "header": "...",
    "transactions": [...],
    "timestamp": 1701388800.0
  },
  "proposer_platform": "Thor",
  "signature": "ed25519_signature_hex"
}
```

**Validation**:
- Sequence number is next expected
- View number matches current view
- Proposer is expected for this sequence (round-robin)
- Signature valid (Ed25519 verification)
- Block internally consistent (valid transactions)

---

### Phase 2: Prepare (Validation Agreement)

**Who**: All platforms (including proposer)

**What**:
1. Each platform validates block from Phase 1
2. If valid, broadcast `PREPARE` message
3. Wait for 2f+1 PREPARE messages (quorum)

**Message Format**:
```json
{
  "type": "PREPARE",
  "view": 0,
  "sequence": 42,
  "block_hash": "sha256_of_block",
  "platform": "Sprout",
  "signature": "ed25519_signature_hex"
}
```

**Quorum**: Need 2f+1 PREPARE messages (including own)
- 4 platforms (f=1): need 3 PREPARE messages
- 7 platforms (f=2): need 5 PREPARE messages

**Meaning**: "I validated this block and think we should commit it"

---

### Phase 3: Commit (Final Agreement)

**Who**: All platforms (after seeing quorum)

**What**:
1. After receiving 2f+1 PREPARE messages, broadcast `COMMIT` message
2. Wait for 2f+1 COMMIT messages (quorum)
3. Execute block (add to local blockchain)

**Message Format**:
```json
{
  "type": "COMMIT",
  "view": 0,
  "sequence": 42,
  "block_hash": "sha256_of_block",
  "platform": "Legion",
  "signature": "ed25519_signature_hex"
}
```

**Quorum**: Need 2f+1 COMMIT messages (including own)

**Meaning**: "I saw a quorum of PREPARE messages, we have consensus"

**Action**: Add block to local blockchain, increment sequence number

---

## Block Proposer Rotation

**Mechanism**: Round-robin deterministic rotation

**Algorithm**:
```python
def get_proposer(sequence: int, platforms: List[str]) -> str:
    """Deterministic proposer selection."""
    return platforms[sequence % len(platforms)]
```

**Example** (4 platforms: Thor, Sprout, Legion, Platform2):
- Block 0: Thor proposes
- Block 1: Sprout proposes
- Block 2: Legion proposes
- Block 3: Platform2 proposes
- Block 4: Thor proposes (cycle repeats)

**Benefits**:
- Fair (each platform gets equal opportunity)
- Deterministic (all platforms agree on who should propose)
- Simple (no leader election needed)

---

## View Changes (Fault Recovery)

### When to Trigger View Change

**Timeout**: If no block committed within timeout period (e.g., 30 seconds)

**Reasons**:
- Primary (proposer) crashed
- Primary is Byzantine (malicious)
- Network partition

### View Change Protocol

**Steps**:
1. Platform detects timeout â†’ broadcast `VIEW-CHANGE` message
2. Wait for 2f+1 VIEW-CHANGE messages
3. New primary selected: `proposer = platforms[view % len(platforms)]`
4. New primary collects latest state from 2f+1 platforms
5. Resume normal operation in new view

**Message Format**:
```json
{
  "type": "VIEW-CHANGE",
  "new_view": 1,
  "last_sequence": 42,
  "platform": "Sprout",
  "signature": "ed25519_signature_hex"
}
```

---

## Message Authentication

**All messages signed with Ed25519**:
- PRE-PREPARE: Signed by proposer
- PREPARE: Signed by validator
- COMMIT: Signed by committer
- VIEW-CHANGE: Signed by requester

**Verification**: Use SageBlockVerifier + SignatureRegistry (Session #42 infrastructure)

**Security Properties**:
- **Authenticity**: Know who sent each message
- **Integrity**: Detect tampering
- **Non-repudiation**: Platforms can't deny sending messages

---

## Network Communication

**Transport**: HTTP/REST (Phase 3 Federation Network)

**Endpoints**:
```
POST /consensus/pre_prepare   # Receive block proposal
POST /consensus/prepare       # Receive validation vote
POST /consensus/commit        # Receive commit vote
POST /consensus/view_change   # Receive view change request
GET  /consensus/status        # Query consensus state
```

**Message Propagation**: Broadcast to all platforms in consensus group

**Implementation**: Use FederationClient (Session #42 infrastructure)

---

## State Machine

**Platform State**:
```python
@dataclass
class ConsensusState:
    view: int = 0  # Current view number
    sequence: int = 0  # Next block to commit
    platforms: List[str] = field(default_factory=list)  # Consensus group
    f: int = 0  # Maximum Byzantine faults (computed from len(platforms))

    # Per-sequence state
    pre_prepare_received: Dict[int, PrePrepareMessage] = field(default_factory=dict)
    prepare_votes: Dict[int, List[PrepareMessage]] = field(default_factory=dict)
    commit_votes: Dict[int, List[CommitMessage]] = field(default_factory=dict)

    # Committed blocks
    committed_blocks: List[Block] = field(default_factory=list)
```

**Transitions**:
1. `IDLE` â†’ `PRE-PREPARED`: Receive valid PRE-PREPARE
2. `PRE-PREPARED` â†’ `PREPARED`: Send PREPARE, receive 2f+1 PREPARE
3. `PREPARED` â†’ `COMMITTED`: Send COMMIT, receive 2f+1 COMMIT
4. `COMMITTED` â†’ `IDLE`: Execute block, increment sequence

---

## Failure Scenarios and Handling

### Scenario 1: Proposer Crashes

**What Happens**:
- Proposer (Thor) crashes before sending PRE-PREPARE
- Other platforms timeout waiting for block
- Trigger VIEW-CHANGE

**Resolution**:
- View changes from 0 to 1
- New proposer selected (next in rotation)
- Resume normal operation

**Time to Recover**: One timeout period (~30 seconds)

---

### Scenario 2: Byzantine Proposer

**What Happens**:
- Proposer (Thor) sends different blocks to different platforms (equivocation)
- Some platforms see block A, others see block B
- Cannot achieve 2f+1 quorum on either block

**Resolution**:
- Timeout triggers VIEW-CHANGE
- New proposer selected
- Byzantine proposer detected (can be logged/penalized)

**Detection**: Platforms share PRE-PREPARE messages, see conflicting signatures

---

### Scenario 3: Network Partition

**What Happens**:
- Network splits into two groups: {Thor, Sprout} and {Legion, Platform2}
- Neither group has 2f+1 quorum (in f=1 configuration)
- No blocks committed

**Resolution**:
- When partition heals, platforms sync state
- Resume from latest committed block
- No fork created (safety preserved)

**Trade-off**: Availability sacrificed for safety (cannot make progress during partition)

---

### Scenario 4: Slow Platform

**What Happens**:
- Platform (Sprout) is slow to respond (network lag, high load)
- Other 3 platforms achieve 2f+1 quorum without Sprout
- Consensus proceeds

**Resolution**:
- Sprout eventually catches up (reads committed blocks)
- No impact on consensus progress

**Benefit**: System tolerates slow platforms (liveness preserved)

---

## Integration with Web4 Blockchain

### Current Architecture (Session #42)

**Per-Society Blockchain**:
```python
@dataclass
class Society:
    society_lct: str
    blocks: List[Dict[str, Any]] = field(default_factory=list)
    # ...
```

**Block Structure**:
```python
{
    "header": {
        "block_number": 0,
        "timestamp": 1701388800.0,
        "prev_hash": "0000...",
        "merkle_root": "abcd..."
    },
    "transactions": [...],
    "signature": "ed25519_signature_hex",
    "platform": "Thor"
}
```

---

### Consensus Integration

**Distributed Blockchain**:
```python
@dataclass
class DistributedSociety(Society):
    """Society with distributed consensus"""

    # Consensus configuration
    consensus_group: List[str] = field(default_factory=list)  # Platform names
    consensus_state: ConsensusState = field(default_factory=ConsensusState)

    # Consensus required flag
    requires_consensus: bool = True
```

**Block Creation Flow**:
```python
# OLD (single platform):
block = create_block(transactions)
block["signature"] = sign(block["header"])
society.blocks.append(block)

# NEW (distributed consensus):
block = create_block(transactions)
block["signature"] = sign(block["header"])
# Send PRE-PREPARE to all platforms
# Wait for PREPARE quorum
# Wait for COMMIT quorum
# Then append to society.blocks
```

---

## Performance Analysis

### Latency

**Best Case** (all platforms responsive, no faults):
- Phase 1 (PRE-PREPARE): 1 network RTT
- Phase 2 (PREPARE): 1 network RTT
- Phase 3 (COMMIT): 1 network RTT
- **Total**: 3 network RTTs

**Example** (10ms RTT between platforms):
- Block latency: 30ms (very fast!)

**Comparison**:
- Bitcoin: ~10 minutes (probabilistic finality)
- Ethereum: ~12 seconds (slot time)
- FB-PBFT: ~30ms (deterministic finality)

---

### Throughput

**Limited by**:
1. Proposer block creation rate
2. Network bandwidth
3. Signature verification speed (0.06ms from Session #42)

**Expected Throughput** (conservative estimate):
- Block creation: 100ms
- 3 RTTs: 30ms
- Total: 130ms per block
- **Throughput**: ~7.7 blocks/second

**Transactions per Block**: Depends on block size (flexible)

**Comparison to Session #42**:
- Single platform: 16,667 signatures/second capacity
- Distributed (f=1): 7.7 blocks/second = ~77 transactions/second (if 10 tx/block)
- **Trade-off**: Consensus adds latency but enables trust-minimized coordination

---

### Scalability

**Consensus Group Size**:
- Small (4 platforms, f=1): Fast, low overhead
- Medium (7 platforms, f=2): Moderate latency
- Large (10+ platforms, f=3+): Higher latency, more messages

**Message Complexity**: O(NÂ²) per consensus round
- 4 platforms: 16 messages
- 7 platforms: 49 messages
- 10 platforms: 100 messages

**Practical Limit**: 10-20 platforms per consensus group (PBFT-like protocols)

**Mitigation**: Use federation structure (multiple consensus groups, not global consensus)

---

## Security Analysis

### Threat Model

**Assumptions**:
1. Up to f platforms are Byzantine (malicious/compromised)
2. Network is asynchronous but eventually delivers messages
3. Cryptographic primitives secure (Ed25519)

**Guarantees**:
1. **Safety**: No two platforms commit conflicting blocks (even with f faults)
2. **Liveness**: System makes progress if â‰¤ f faults and network eventually delivers

---

### Attack Scenarios

#### Attack 1: Double-Spend via Equivocation

**Attack**: Byzantine proposer sends different blocks to different platforms

**Defense**:
- Platforms detect conflicting PRE-PREPARE messages
- Cannot achieve 2f+1 quorum on either block
- View change triggered, Byzantine proposer bypassed

**Result**: Attack fails (safety preserved)

---

#### Attack 2: Denial of Service

**Attack**: Byzantine platform refuses to participate (doesn't send PREPARE/COMMIT)

**Defense**:
- Quorum is 2f+1, can tolerate f non-participating platforms
- If Byzantine platform is proposer, view change after timeout

**Result**: System continues (liveness preserved up to f faults)

---

#### Attack 3: Long-Range Attack

**Attack**: Compromised platforms rewrite history (create alternate chain from genesis)

**Defense**:
- Finality is deterministic (2f+1 signatures on COMMIT)
- Cannot revert committed blocks without compromising 2f+1 platforms
- With f=1 (4 platforms), need to compromise 3 platforms (75%)

**Result**: Attack very expensive (must compromise 2/3+ of consensus group)

---

#### Attack 4: Sybil Attack

**Attack**: Attacker creates many fake platform identities

**Defense**:
- Consensus group explicitly configured (platforms must be invited)
- Each platform has hardware-bound identity (Ed25519 keys)
- Cannot join consensus without existing platforms' approval

**Result**: Attack blocked (permissioned consensus group)

---

## Implementation Plan

### Phase 1: Core Protocol (Session #43)

**Deliverables**:
1. `ConsensusState` dataclass
2. Message types (PRE-PREPARE, PREPARE, COMMIT)
3. State machine logic (phase transitions)
4. Message validation (signature verification)

**Testing**: Simulated platforms (in-process, like Session #42)

**Estimated Time**: 3-4 hours

---

### Phase 2: Network Integration (Session #44)

**Deliverables**:
1. HTTP endpoints for consensus messages
2. Integration with FederationServer/FederationClient
3. Message broadcasting logic
4. View change protocol

**Testing**: Real HTTP communication between platforms (if hardware available)

**Estimated Time**: 3-4 hours

---

### Phase 3: Blockchain Integration (Session #45)

**Deliverables**:
1. `DistributedSociety` class
2. Consensus-aware block creation
3. Committed block execution
4. State synchronization between platforms

**Testing**: Multi-platform demos (2-4 platforms)

**Estimated Time**: 3-4 hours

---

### Phase 4: Fault Tolerance Testing (Session #46)

**Deliverables**:
1. Crash fault injection
2. Byzantine fault injection (equivocation, withholding)
3. Network partition simulation
4. Recovery validation

**Testing**: Adversarial scenarios, measure recovery time

**Estimated Time**: 4-6 hours

---

## Open Research Questions

### Question 1: How to Bootstrap Consensus Group?

**Current**: Consensus group configured statically (list of platforms)

**Needed**: Dynamic membership (platforms join/leave)

**Approaches**:
- Reconfiguration protocol (add/remove platforms via consensus)
- Governance mechanism (voting on membership changes)
- Stake-based entry (require ATP stake to join)

**Priority**: Medium (static works for research prototype)

---

### Question 2: How to Incentivize Honest Participation?

**Current**: Platforms participate altruistically

**Needed**: Economic incentives for honest behavior

**Approaches**:
- ATP rewards for block proposers (like mining rewards)
- Slashing for detected Byzantine behavior
- Reputation scores affecting future selection

**Priority**: Medium (ties into ATP economics)

---

### Question 3: How to Handle Cross-Society Consensus?

**Current**: Each society has independent blockchain

**Needed**: Cross-society coordination

**Approaches**:
- Separate consensus groups per society
- Global consensus for cross-society transactions
- Nested consensus (society-level â†’ federation-level)

**Priority**: Low (single society works for now)

---

### Question 4: How to Optimize for Low-Latency Networks?

**Current**: Designed for WAN (wide-area network) with RTT up to 100ms

**Observation**: If platforms on same LAN, RTT could be < 1ms

**Question**: Can we do better than 3 RTTs?

**Approaches**:
- Speculative execution (execute before COMMIT quorum)
- Optimistic fast path (skip PREPARE if all platforms agree)
- Pipelining (start next consensus while finishing previous)

**Priority**: Low (3 RTTs is already very fast)

---

## Comparison to Existing Consensus Protocols

| Protocol | Type | Fault Tolerance | Finality | Latency | Complexity |
|----------|------|-----------------|----------|---------|------------|
| **Bitcoin** | PoW | 0-50% hashrate | Probabilistic (6 blocks) | ~60 min | Low |
| **Ethereum** | PoS | 1/3 validators | Probabilistic (2 epochs) | ~12 min | High |
| **PBFT** | BFT | f < N/3 | Deterministic | ~3 RTTs | High |
| **Raft** | CFT | f < N/2 | Deterministic | ~2 RTTs | Medium |
| **FB-PBFT** | BFT | f < N/3 | Deterministic | ~3 RTTs | Medium |

**FB-PBFT Advantages**:
- Deterministic finality (unlike PoW/PoS)
- Byzantine fault tolerant (unlike Raft)
- Simpler than full PBFT (federation context)
- Fast (3 RTT latency)

**FB-PBFT Trade-offs**:
- Limited scalability (10-20 platforms)
- Requires 2/3 honest majority (stricter than Raft's 1/2)
- Not permissionless (consensus group explicitly configured)

---

## Honest Assessment

### What This Achieves âœ…

1. **Complete protocol specification**: All phases, messages, state transitions defined
2. **Byzantine fault tolerance**: Can handle up to f malicious platforms
3. **Builds on Session #42**: Leverages Ed25519 verification infrastructure
4. **Incremental deployment**: Can start with 2 platforms, scale to N
5. **Clear implementation plan**: Broken into 4 phases

### What This Doesn't Achieve âŒ

1. **Not implemented yet**: This is a design document, code doesn't exist
2. **Not tested**: No empirical validation of fault tolerance claims
3. **Not production-deployed**: Research prototype design only
4. **Not optimized**: Many optimization opportunities (pipelining, fast path, etc.)
5. **No dynamic membership**: Consensus group statically configured

### Research Gaps ðŸ”¬

1. **Empirical latency**: Need real measurements across platforms
2. **Byzantine fault testing**: Need to inject actual malicious behavior
3. **Network partition recovery**: Need to test partition scenarios
4. **Cross-society coordination**: How does this scale to multiple societies?
5. **Economic incentives**: How to align participation with ATP economics?

### Fair Assessment

**Status**: Complete protocol design for research prototype distributed consensus

**Readiness**: Ready for implementation and simulation testing (Phase 1)

**Next Step**: Implement core protocol (ConsensusState + message validation)

**Timeline**: 4-6 hours to working simulated consensus (2-4 platforms)

---

## References

**PBFT Paper**: Castro & Liskov (1999) - "Practical Byzantine Fault Tolerance"

**Session #42**: Cross-platform block verification (Ed25519)

**Phase 3 Federation Network**: HTTP/REST infrastructure (Thor)

**Web4 Blockchain**: Society model with per-society blocks

---

## Summary

**FB-PBFT (Federation-Based PBFT-Lite)** provides:
- Distributed consensus for Web4 societies
- Byzantine fault tolerance (up to f faults in 3f+1 configuration)
- Deterministic finality (3 RTT latency)
- Built on Session #42's cross-platform verification infrastructure
- Clear path from design â†’ implementation â†’ testing â†’ deployment

**Key Insight**: Cross-platform block verification (Session #42) is the cryptographic foundation. Consensus protocol (this session) is the coordination layer built on that foundation.

**Next Action**: Implement Phase 1 (core protocol) with simulation testing.

---

**Status**: Research prototype design - ready for implementation
**Implementation Priority**: Phase 1 (core protocol) - 3-4 hours
**Testing Approach**: Simulated platforms first, real hardware later
**Success Criteria**: 2-4 simulated platforms reach consensus on block sequence

Co-Authored-By: Claude (Legion Autonomous) <noreply@anthropic.com>
